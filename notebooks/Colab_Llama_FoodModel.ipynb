{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4385914e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Install Dependencies\n",
    "!pip install -q transformers accelerate torch bitsandbytes\n",
    "!pip install -q flask flask-cors pyngrok\n",
    "!pip install -q huggingface-hub\n",
    "print(\"‚úÖ Dependencies installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203108eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Setup Ngrok & Hugging Face\n",
    "# Get ngrok token from https://dashboard.ngrok.com/get-started/your-authtoken\n",
    "# Get HF token from https://huggingface.co/settings/tokens\n",
    "NGROK_TOKEN = \"YOUR_NGROK_TOKEN_HERE\"  # ‚ö†Ô∏è REPLACE THIS\n",
    "HF_TOKEN = \"YOUR_HUGGINGFACE_TOKEN_HERE\"  # ‚ö†Ô∏è REPLACE THIS (optional but recommended)\n",
    "\n",
    "!ngrok authtoken {NGROK_TOKEN}\n",
    "print(\"‚úÖ Ngrok configured\")\n",
    "\n",
    "# Login to Hugging Face (optional - needed for some gated models)\n",
    "try:\n",
    "    from huggingface_hub import login\n",
    "    if HF_TOKEN != \"YOUR_HUGGINGFACE_TOKEN_HERE\":\n",
    "        login(token=HF_TOKEN)\n",
    "        print(\"‚úÖ Hugging Face authenticated\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Hugging Face token not provided (skipping login)\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è HF login skipped: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d14f030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Create Flask Server with Llama Model\n",
    "%%writefile llama_server.py\n",
    "\n",
    "from flask import Flask, request, jsonify\n",
    "from flask_cors import CORS\n",
    "from pyngrok import ngrok\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "app = Flask(__name__)\n",
    "CORS(app)\n",
    "\n",
    "model = None\n",
    "tokenizer = None\n",
    "\n",
    "def load_model():\n",
    "    global model, tokenizer\n",
    "    logger.info(\"üîÑ Loading FortyMiles Llama-3 Food/Nutrition Model...\")\n",
    "    \n",
    "    try:\n",
    "        # Use the 10-epoch trained model (best performance)\n",
    "        model_name = \"fortymiles/Llama-3-8B-sft-lora-food-nutrition-10-epoch\"\n",
    "        \n",
    "        # Load tokenizer\n",
    "        logger.info(\"Loading tokenizer...\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        \n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "        \n",
    "        # Load model with 4-bit quantization for faster inference\n",
    "        logger.info(\"Loading model with 4-bit quantization...\")\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.float16,\n",
    "            bnb_4bit_use_double_quant=True\n",
    "        )\n",
    "        \n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            quantization_config=bnb_config,\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        model.eval()\n",
    "        \n",
    "        device = next(model.parameters()).device\n",
    "        logger.info(f\"‚úÖ Llama-3 Food/Nutrition Model loaded successfully on {device}!\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Failed to load model: {e}\")\n",
    "        return False\n",
    "\n",
    "@app.route('/health', methods=['GET'])\n",
    "def health():\n",
    "    return jsonify({\n",
    "        \"status\": \"healthy\",\n",
    "        \"model_loaded\": model is not None,\n",
    "        \"model_name\": \"fortymiles/Llama-3-8B-sft-lora-food-nutrition-10-epoch\",\n",
    "        \"device\": str(next(model.parameters()).device) if model else \"not loaded\"\n",
    "    })\n",
    "\n",
    "@app.route('/generate', methods=['POST'])\n",
    "def generate():\n",
    "    global model, tokenizer\n",
    "    \n",
    "    if model is None:\n",
    "        return jsonify({\"error\": \"Model not loaded\"}), 500\n",
    "    \n",
    "    try:\n",
    "        data = request.json\n",
    "        prompt = data.get('prompt', '')\n",
    "        max_tokens = data.get('max_tokens', 1024)\n",
    "        temperature = data.get('temperature', 0.7)\n",
    "        top_p = data.get('top_p', 0.9)\n",
    "        \n",
    "        logger.info(f\"üìù Generating (max_tokens={max_tokens})...\")\n",
    "        \n",
    "        # Tokenize\n",
    "        inputs = tokenizer(\n",
    "            prompt,\n",
    "            return_tensors=\"pt\",\n",
    "            max_length=2048,\n",
    "            truncation=True\n",
    "        )\n",
    "        \n",
    "        device = next(model.parameters()).device\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        # Generate\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_tokens,\n",
    "                temperature=temperature,\n",
    "                top_p=top_p,\n",
    "                do_sample=True,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                repetition_penalty=1.1\n",
    "            )\n",
    "        \n",
    "        # Decode\n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Extract response part (after ### Response:)\n",
    "        if \"### Response:\" in response:\n",
    "            response = response.split(\"### Response:\")[-1].strip()\n",
    "        elif prompt in response:\n",
    "            # Remove the prompt from response\n",
    "            response = response.replace(prompt, \"\").strip()\n",
    "        \n",
    "        logger.info(f\"‚úÖ Generated {len(response)} characters\")\n",
    "        \n",
    "        return jsonify({\n",
    "            \"status\": \"success\",\n",
    "            \"response\": response,\n",
    "            \"response_length\": len(response)\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Error: {e}\")\n",
    "        return jsonify({\"error\": str(e)}), 500\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    if load_model():\n",
    "        public_url = ngrok.connect(5000)\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"üåê PUBLIC URL (COPY THIS):\")\n",
    "        print(f\"   {public_url}\")\n",
    "        print(\"=\"*60)\n",
    "        print(\"\\nüìù Update your local ml_recommender.py:\")\n",
    "        print(f'   USE_COLAB = True')\n",
    "        print(f'   COLAB_API_URL = \"{public_url}\"')\n",
    "        print(\"\\nüöÄ Server starting...\\n\")\n",
    "        app.run(host='0.0.0.0', port=5000)\n",
    "    else:\n",
    "        print(\"‚ùå Failed to load model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b314b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Start Server (KEEP THIS RUNNING)\n",
    "# This cell will run continuously. Don't stop it!\n",
    "# Model download: ~16GB (first time only, then cached)\n",
    "# Loading time: 3-5 minutes\n",
    "!python llama_server.py"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
