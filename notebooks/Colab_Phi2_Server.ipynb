{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca83816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Install Dependencies\n",
    "!pip install -q transformers peft accelerate torch\n",
    "!pip install -q flask flask-cors pyngrok\n",
    "print(\"‚úÖ Dependencies installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd55acf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Upload LoRA Checkpoint\n",
    "# Option A: From Google Drive (recommended)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Copy your checkpoint-224 folder from Drive to Colab\n",
    "# Adjust path to where you uploaded checkpoint-224 in your Drive\n",
    "!cp -r \"/content/drive/MyDrive/Diet-Plan-AI/checkpoint-224\" /content/\n",
    "\n",
    "# Verify checkpoint exists\n",
    "import os\n",
    "if os.path.exists('/content/checkpoint-224'):\n",
    "    print(\"‚úÖ Checkpoint-224 found!\")\n",
    "    !ls -lh /content/checkpoint-224\n",
    "else:\n",
    "    print(\"‚ùå Checkpoint not found. Please upload it to Google Drive first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ec48e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Setup Ngrok\n",
    "# Get your token from https://dashboard.ngrok.com/get-started/your-authtoken\n",
    "NGROK_TOKEN = \"YOUR_NGROK_TOKEN_HERE\"  # ‚ö†Ô∏è REPLACE THIS\n",
    "\n",
    "!ngrok authtoken {NGROK_TOKEN}\n",
    "print(\"‚úÖ Ngrok configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ae1130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Create Flask Server\n",
    "%%writefile colab_server.py\n",
    "\n",
    "from flask import Flask, request, jsonify\n",
    "from flask_cors import CORS\n",
    "from pyngrok import ngrok\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "app = Flask(__name__)\n",
    "CORS(app)\n",
    "\n",
    "model = None\n",
    "tokenizer = None\n",
    "\n",
    "def load_model():\n",
    "    global model, tokenizer\n",
    "    logger.info(\"üîÑ Loading fine-tuned Phi-2 model...\")\n",
    "    \n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            \"microsoft/phi-2\",\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "        \n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            \"microsoft/phi-2\",\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        \n",
    "        model = PeftModel.from_pretrained(\n",
    "            model,\n",
    "            \"/content/checkpoint-224\",\n",
    "            is_trainable=False\n",
    "        )\n",
    "        model.eval()\n",
    "        \n",
    "        device = next(model.parameters()).device\n",
    "        logger.info(f\"‚úÖ Model loaded successfully on {device}!\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Failed to load model: {e}\")\n",
    "        return False\n",
    "\n",
    "@app.route('/health', methods=['GET'])\n",
    "def health():\n",
    "    return jsonify({\n",
    "        \"status\": \"healthy\",\n",
    "        \"model_loaded\": model is not None,\n",
    "        \"device\": str(next(model.parameters()).device) if model else \"not loaded\"\n",
    "    })\n",
    "\n",
    "@app.route('/generate', methods=['POST'])\n",
    "def generate():\n",
    "    global model, tokenizer\n",
    "    \n",
    "    if model is None:\n",
    "        return jsonify({\"error\": \"Model not loaded\"}), 500\n",
    "    \n",
    "    try:\n",
    "        data = request.json\n",
    "        prompt = data.get('prompt', '')\n",
    "        max_tokens = data.get('max_tokens', 800)\n",
    "        temperature = data.get('temperature', 0.7)\n",
    "        top_p = data.get('top_p', 0.9)\n",
    "        \n",
    "        logger.info(f\"üìù Generating (max_tokens={max_tokens})...\")\n",
    "        \n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=2048, truncation=True)\n",
    "        device = next(model.parameters()).device\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_tokens,\n",
    "                temperature=temperature,\n",
    "                top_p=top_p,\n",
    "                do_sample=True,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        if \"### Response:\" in response:\n",
    "            response = response.split(\"### Response:\")[-1].strip()\n",
    "        \n",
    "        logger.info(f\"‚úÖ Generated {len(response)} characters\")\n",
    "        \n",
    "        return jsonify({\n",
    "            \"status\": \"success\",\n",
    "            \"response\": response,\n",
    "            \"response_length\": len(response)\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Error: {e}\")\n",
    "        return jsonify({\"error\": str(e)}), 500\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    if load_model():\n",
    "        public_url = ngrok.connect(5000)\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"üåê PUBLIC URL (COPY THIS):\")\n",
    "        print(f\"   {public_url}\")\n",
    "        print(\"=\"*60)\n",
    "        print(\"\\nüìù Update your local ml_recommender.py:\")\n",
    "        print(f'   USE_COLAB = True')\n",
    "        print(f'   COLAB_API_URL = \"{public_url}\"')\n",
    "        print(\"\\nüöÄ Server starting...\\n\")\n",
    "        app.run(host='0.0.0.0', port=5000)\n",
    "    else:\n",
    "        print(\"‚ùå Failed to load model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab07b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Start Server (KEEP THIS RUNNING)\n",
    "# This cell will run continuously. Don't stop it!\n",
    "!python colab_server.py"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
